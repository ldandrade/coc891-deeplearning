{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Matching Networks Architecture implementation consists of the following 5 important parts:\n",
    "\n",
    "- Embeddings Extractor, g.\n",
    "- Full Context Embeddings, Bi-directional LSTM, f\n",
    "- Cosine Similarity Distance Function, c\n",
    "- Attention Model: Softmax(c)\n",
    "- Loss Function: Cross Entropy Loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tqdm\n",
    "!pip3 install matplotlib\n",
    "!pip3 install jetson-stats\n",
    "!pip3 install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "from torchsummary import summary\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load omniglot dataset, tranformed in .npy format using helper script.\n",
    "#In helper script, we are just loading data in size format: [total_number,character,28,28]. For more details, go through helper.py script.\n",
    "\n",
    "x = np.load('data/data.npy') # Load Data\n",
    "print (x.shape)\n",
    "print (type(x))\n",
    "x = np.reshape(x, newshape=(x.shape[0], x.shape[1], 28, 28, 1)) # expand dimension from (x.shape[0],x.shape[1],28,28)\n",
    "np.random.shuffle(x) # shuffle dataset\n",
    "x_train, x_val, x_test = x[:1200], x[1200:1411], x[1411:] # divide dataset in to train, val,ctest\n",
    "batch_size = 16 # setting batch_size\n",
    "n_classes = x.shape[0] # total number of classes\n",
    "classes_per_set = 20 # Number of classes per set\n",
    "samples_per_class = 1 # as we are choosing it to be one shot learning, so we have 1 sample\n",
    "print (x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Images: Here we have use normalization method.\n",
    "\n",
    "def processes_batch(data, mu, sigma):\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "# Normalize Dataset\n",
    "x_train = processes_batch(x_train, np.mean(x_train), np.std(x_train))\n",
    "x_val = processes_batch(x_val, np.mean(x_val), np.std(x_val))\n",
    "x_test = processes_batch(x_test, np.mean(x_test), np.std(x_test))\n",
    "\n",
    "# Defining dictionary of dataset\n",
    "datatset = {\"train\": x_train, \"val\": x_val, \"test\": x_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Visualize example of one character written by 20 people.\n",
    "\n",
    "temp = x_train[0,:,:,:,:]  \n",
    "for i in range(0,20):\n",
    "    plt.figure()\n",
    "    plt.imshow(temp[i,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Training Data Processing\n",
    "#To Load Omniglot dataset, and prepare it for Matching Networks Architecture, we need to create:\n",
    "\n",
    "#Label Set: Variable choose_label\n",
    "#Support Set: support_set_x, support_set_y\n",
    "#Batch from Suppport Set Examples\n",
    "#Let's first create a batch which can give a support set, and target set.\n",
    "\n",
    "def sample_batch(data):\n",
    "        \"\"\"\n",
    "        Generates sample batch \n",
    "        :param : data - one of(train,test,val) our current dataset shape [total_classes,20,28,28,1]\n",
    "        :return: [support_set_x,support_set_y,target_x,target_y] for Matching Networks\n",
    "        \"\"\"\n",
    "        support_set_x = np.zeros((batch_size, classes_per_set, samples_per_class, data.shape[2],\n",
    "                                  data.shape[3], data.shape[4]), np.float32)\n",
    "        support_set_y = np.zeros((batch_size, classes_per_set, samples_per_class), np.int32)\n",
    "        \n",
    "        target_x = np.zeros((batch_size, data.shape[2], data.shape[3], data.shape[4]), np.float32)\n",
    "        target_y = np.zeros((batch_size, 1), np.int32)\n",
    "        for i in range(batch_size):\n",
    "            choose_classes = np.random.choice(data.shape[0], size=classes_per_set, replace=False) # choosing random classes\n",
    "            choose_label = np.random.choice(classes_per_set, size=1) # label set\n",
    "            choose_samples = np.random.choice(data.shape[1], size=samples_per_class + 1, replace=False)\n",
    "            x_temp = data[choose_classes] # choosing classes\n",
    "            x_temp = x_temp[:, choose_samples] # choosing sample batch from classes chosen outputs 20X2X28X28X1\n",
    "            y_temp = np.arange(classes_per_set) # will return [0,1,2,3,...,19]\n",
    "            support_set_x[i] = x_temp[:, :-1]\n",
    "            support_set_y[i] = np.expand_dims(y_temp[:], axis=1) # expand dimension\n",
    "            target_x[i] = x_temp[choose_label, -1]\n",
    "            target_y[i] = y_temp[choose_label]\n",
    "        return support_set_x, support_set_y, target_x, target_y # returns support of [batch_size, 20 classes per set, 1 sample, 28, 28,1]\n",
    "    \n",
    "def get_batch(dataset_name):\n",
    "        \"\"\"\n",
    "        gen batch while training\n",
    "        :param dataset_name: The name of dataset(one of \"train\",\"val\",\"test\")\n",
    "        :return: a batch images\n",
    "        \"\"\"\n",
    "        support_set_x, support_set_y, target_x, target_y = sample_batch(datatset[dataset_name])\n",
    "        support_set_x = support_set_x.reshape((support_set_x.shape[0], support_set_x.shape[1] * support_set_x.shape[2],\n",
    "                                               support_set_x.shape[3], support_set_x.shape[4], support_set_x.shape[5]))\n",
    "        support_set_y = support_set_y.reshape(support_set_y.shape[0], support_set_y.shape[1] * support_set_y.shape[2])\n",
    "        return support_set_x, support_set_y, target_x, target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Create an Embeddings extractor\n",
    "\n",
    "def convLayer(in_channels, out_channels, dropout_prob=0.0):\n",
    "    \"\"\"\n",
    "    :param dataset_name: The name of dataset(one of \"train\",\"val\",\"test\")\n",
    "    :return: a batch images\n",
    "    \"\"\"\n",
    "    cnn_seq = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout(dropout_prob)\n",
    "    )\n",
    "    return cnn_seq\n",
    "\n",
    "class Embeddings_extractor(nn.Module):\n",
    "    def __init__(self, layer_size=64, num_channels=1, dropout_prob=0.5, image_size=28):\n",
    "        super(Embeddings_extractor, self).__init__()\n",
    "        \"\"\"\n",
    "        Build a CNN to produce embeddings\n",
    "        :param layer_size:64(default)\n",
    "        :param num_channels:\n",
    "        :param keep_prob:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        self.layer1 = convLayer(num_channels, layer_size, dropout_prob)\n",
    "        self.layer2 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "        self.layer3 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "        self.layer4 = convLayer(layer_size, layer_size, dropout_prob)\n",
    "\n",
    "        finalSize = int(math.floor(image_size / (2 * 2 * 2 * 2)))\n",
    "        self.outSize = finalSize * finalSize * layer_size\n",
    "\n",
    "    def forward(self, image_input):\n",
    "        \"\"\"\n",
    "        :param: Image\n",
    "        :return: embeddings\n",
    "        \"\"\"\n",
    "        x = self.layer1(image_input)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5: Create an Attention model after classifier.\n",
    "\n",
    "class AttentionalClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionalClassify, self).__init__()\n",
    "\n",
    "    def forward(self, similarities, support_set_y):\n",
    "        \"\"\"\n",
    "        Products pdfs over the support set classes for the target set image.\n",
    "        :param similarities: A tensor with cosine similarites of size[batch_size,sequence_length]\n",
    "        :param support_set_y:[batch_size,sequence_length,classes_num]\n",
    "        :return: Softmax pdf shape[batch_size,classes_num]\n",
    "        \"\"\"\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        softmax_similarities = softmax(similarities)\n",
    "        preds = softmax_similarities.unsqueeze(1).bmm(support_set_y).squeeze()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Create a Distance Network, which will take output from Test Image, and training embeddings, to calculate the Distance.\n",
    "#find cosine similarities between support set and input_test_image\n",
    "\n",
    "class DistanceNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This model calculates the cosine distance between each of the support set embeddings and \n",
    "    the target image embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistanceNetwork, self).__init__()\n",
    "\n",
    "    def forward(self, support_set, input_image):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        :param support_set:the embeddings of the support set images.shape[sequence_length,batch_size,64]\n",
    "        :param input_image: the embedding of the target image,shape[batch_size,64]\n",
    "        :return:shape[batch_size,sequence_length]\n",
    "        \"\"\"\n",
    "        eps = 1e-10\n",
    "        similarities = []\n",
    "        for support_image in support_set:\n",
    "            sum_support = torch.sum(torch.pow(support_image, 2), 1)\n",
    "            support_manitude = sum_support.clamp(eps, float(\"inf\")).rsqrt()\n",
    "            dot_product = input_image.unsqueeze(1).bmm(support_image.unsqueeze(2)).squeeze()\n",
    "            cosine_similarity = dot_product * support_manitude\n",
    "            similarities.append(cosine_similarity)\n",
    "        similarities = torch.stack(similarities)\n",
    "        return similarities.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Create a Bi-directional LSTM, which is taking input and output from Test-image, and put them in same embeddings space.\n",
    "#If we wish to use full-context embeddings, Matching Networks introduced Bi-directional LSTM for it.\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, layer_size, batch_size, vector_dim):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        \"\"\"\n",
    "        Initial a muti-layer Bidirectional LSTM\n",
    "        :param layer_size: a list of each layer'size\n",
    "        :param batch_size: \n",
    "        :param vector_dim: \n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = layer_size[0]\n",
    "        self.vector_dim = vector_dim\n",
    "        self.num_layer = len(layer_size)\n",
    "        self.lstm = nn.LSTM(input_size=self.vector_dim, num_layers=self.num_layer, hidden_size=self.hidden_size,\n",
    "                            bidirectional=True)\n",
    "        self.hidden = (Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False),\n",
    "                Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, \n",
    "        to detach them from their history.\"\"\"\n",
    "        if type(h) == torch.Tensor:\n",
    "            return Variable(h.data).to(device)\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.hidden = self.repackage_hidden(self.hidden)\n",
    "        output, self.hidden = self.lstm(inputs, self.hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's club all small modules we made, and create a matching network.Â¶\n",
    "\n",
    "class MatchingNetwork(nn.Module):\n",
    "    def __init__(self, keep_prob, batch_size=32, num_channels=1, learning_rate=1e-3, fce=False, num_classes_per_set=20, \\\n",
    "                 num_samples_per_class=1, image_size=28):\n",
    "        \"\"\"\n",
    "        Matching Network\n",
    "        :param keep_prob: dropout rate\n",
    "        :param batch_size:\n",
    "        :param num_channels:\n",
    "        :param learning_rate:\n",
    "        :param fce: Flag indicating whether to use full context embeddings(i.e. apply an LSTM on the CNN embeddings)\n",
    "        :param num_classes_per_set:\n",
    "        :param num_samples_per_class:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        super(MatchingNetwork, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.num_channels = num_channels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes_per_set = num_classes_per_set\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.image_size = image_size\n",
    "        # Let's set all peices of Matching Networks Architecture\n",
    "        self.g = Embeddings_extractor(layer_size=64, num_channels=num_channels, dropout_prob=keep_prob, image_size=image_size)\n",
    "        self.f = fce # if we are considering full-context embeddings\n",
    "        self.c = DistanceNetwork() # cosine distance among embeddings\n",
    "        self.a = AttentionalClassify() # softmax of cosine distance of embeddings\n",
    "        if self.f:\n",
    "            self.lstm = BidirectionalLSTM(layer_size=[32], batch_size=self.batch_size, vector_dim=self.g.outSize)\n",
    "\n",
    "    def forward(self, support_set_images, support_set_y_one_hot, target_image, target_y):\n",
    "        \"\"\"\n",
    "        Main process of the network\n",
    "        :param support_set_images: shape[batch_size,sequence_length,num_channels,image_size,image_size]\n",
    "        :param support_set_y_one_hot: shape[batch_size,sequence_length,num_classes_per_set]\n",
    "        :param target_image: shape[batch_size,num_channels,image_size,image_size]\n",
    "        :param target_y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # produce embeddings for support set images\n",
    "        encoded_images = []\n",
    "        for i in np.arange(support_set_images.size(1)):\n",
    "            gen_encode = self.g(support_set_images[:, i, :, :])\n",
    "            encoded_images.append(gen_encode)\n",
    "            \n",
    "        # produce embeddings for target images\n",
    "        gen_encode = self.g(target_image)\n",
    "        encoded_images.append(gen_encode)\n",
    "        output = torch.stack(encoded_images,dim=0)\n",
    "        \n",
    "        # if we are considering full-context embeddings\n",
    "        if self.f:\n",
    "            output = self.lstm(output)\n",
    "            \n",
    "        # get similarities between support set embeddings and target\n",
    "        similarites = self.c(support_set=output[:-1], input_image=output[-1])\n",
    "        \n",
    "        # produce predictions for target probabilities\n",
    "        preds = self.a(similarites, support_set_y=support_set_y_one_hot)\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        values, indices = preds.max(1)\n",
    "        accuracy = torch.mean((indices.squeeze() == target_y).float())\n",
    "        crossentropy_loss = F.cross_entropy(preds, target_y.long())\n",
    "\n",
    "        return accuracy, crossentropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9: Create a Dataset Loader.\n",
    "#For our case, as we are using Omniglot Dataset,it will create a Omnligloat builder which calls Matching Network, and run its epochs for training, testing, and validation purpose.\n",
    "\n",
    "def run_epoch(total_train_batches, name='train'):\n",
    "    \"\"\"\n",
    "    Run the training epoch\n",
    "    :param total_train_batches: Number of batches to train on\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    total_c_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    for i in range(int(total_train_batches)):\n",
    "            x_support_set, y_support_set, x_target, y_target = get_batch(name)\n",
    "            x_support_set = Variable(torch.from_numpy(x_support_set)).float()\n",
    "            y_support_set = Variable(torch.from_numpy(y_support_set), requires_grad=False).long()\n",
    "            x_target = Variable(torch.from_numpy(x_target)).float()\n",
    "            y_target = Variable(torch.from_numpy(y_target), requires_grad=False).squeeze().long()\n",
    "\n",
    "            # convert to one hot encoding\n",
    "            y_support_set = y_support_set.unsqueeze(2)\n",
    "            sequence_length = y_support_set.size()[1]\n",
    "            batch_size = y_support_set.size()[0]\n",
    "            y_support_set_one_hot = Variable(torch.zeros(batch_size, sequence_length, classes_per_set).scatter_(2,y_support_set.data,1), requires_grad=False)\n",
    "\n",
    "            # reshape channels and change order\n",
    "            size = x_support_set.size()\n",
    "            x_support_set = x_support_set.permute(0, 1, 4, 2, 3)\n",
    "            x_target = x_target.permute(0, 3, 1, 2)\n",
    "\n",
    "            x_support_set, y_support_set_one_hot, x_target, y_target = x_support_set.to(device), y_support_set_one_hot.to(device), x_target.to(device), y_target.to(device)\n",
    "            \n",
    "            acc, c_loss = matchNet(x_support_set, y_support_set_one_hot, x_target, y_target)\n",
    "\n",
    "            # optimize process\n",
    "            optimizer.zero_grad()\n",
    "            c_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iter_out = \"tr_loss: {}, tr_accuracy: {}\".format(c_loss, acc)\n",
    "            total_c_loss += c_loss\n",
    "            total_accuracy += acc\n",
    "\n",
    "    total_c_loss = total_c_loss / total_train_batches\n",
    "    total_accuracy = total_accuracy / total_train_batches\n",
    "    return total_c_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size=20\n",
    "num_channels=1\n",
    "lr=1e-3\n",
    "image_size=28\n",
    "classes_per_set=20\n",
    "samples_per_class=1\n",
    "keep_prob=0.0\n",
    "fce=True\n",
    "optim=\"adam\"\n",
    "wd=0\n",
    "matchNet = MatchingNetwork(keep_prob, batch_size, num_channels, lr, fce, classes_per_set,\n",
    "                                samples_per_class, image_size)\n",
    "matchNet.to(device)\n",
    "\n",
    "total_iter = 0\n",
    "total_train_iter = 0\n",
    "optimizer = torch.optim.Adam(matchNet.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',verbose=True)\n",
    "\n",
    "# Training setup\n",
    "total_epochs = 100\n",
    "total_train_batches = 10\n",
    "total_val_batches = 5\n",
    "total_test_batches = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(matchNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,train_accuracy=[],[]\n",
    "val_loss,val_accuracy=[],[]\n",
    "test_loss,test_accuracy=[],[]\n",
    "\n",
    "\n",
    "for e in range(total_epochs):\n",
    "    ############################### Training Step ##########################################\n",
    "    total_c_loss, total_accuracy = run_epoch(total_train_batches,'train')\n",
    "    train_loss.append(total_c_loss)\n",
    "    train_accuracy.append(total_accuracy)\n",
    "    \n",
    "    ################################# Validation Step #######################################\n",
    "    total_val_c_loss, total_val_accuracy = run_epoch(total_val_batches, 'val')\n",
    "    val_loss.append(total_val_c_loss)\n",
    "    val_accuracy.append(total_val_accuracy)\n",
    "    print(\"Epoch {}: train_loss:{:.2f} train_accuracy:{:.2f} valid_loss:{:.2f} valid_accuracy:{:.2f}\".\n",
    "          format(e, total_c_loss, total_accuracy, total_val_c_loss, total_val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Let's obtain our test accuracy by running the following code block:\n",
    "\n",
    "total_test_c_loss, total_test_accuracy = run_epoch(total_test_batches,'test')\n",
    "print(\"test_accuracy:{}%\".format(total_test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize our results\n",
    "\n",
    "def plot_loss(train,val,name1=\"train_loss\",name2=\"val_loss\",title=\"\"):\n",
    "    plt.title(title)\n",
    "    plt.plot(train, label=name1)\n",
    "    plt.plot(val, label=name2)\n",
    "    plt.legend()\n",
    "\n",
    "plot_loss(train_loss,val_loss,\"train_loss\",\"val_loss\",\"Loss Graph\")\n",
    "plot_loss(train_accuracy,val_accuracy,\"train_accuracy\",\"val_accuracy\",\"Accuracy Graph\")\n"
   ]
  }
 ]
}