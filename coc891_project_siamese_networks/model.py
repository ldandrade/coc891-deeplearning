import torch
import torch.nn as nn
import torch.nn.functional as F


class SiameseNet(nn.Module):
    """
    A Convolutional Siamese Network for One-Shot Learning [1].
    Siamese networts learn image representations via a supervised metric-based
    approach. Once tuned, their learned features can be leveraged for one-shot
    learning without any retraining.
    References
    ----------
    - Koch et al., https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf
    """
    def __init__(self):
        super(SiameseNet, self).__init__()

        #self.conv1 = nn.Conv2d(1, 64, 10) #input_channel, out_channel (n filters), kernel_size
        #self.conv2 = nn.Conv2d(64, 128, 7)
        #self.conv3 = nn.Conv2d(128, 128, 4)
        #self.conv4 = nn.Conv2d(128, 256, 4)
        #self.fc1 = nn.Linear(9216, 4096)
        #self.fc2 = nn.Linear(4096, 1)
        
        #self.conv1 = nn.Conv2d(1, 32, 10) #input_channel, out_channel (n filters), kernel_size
        #self.conv2 = nn.Conv2d(32, 64, 7)
        #self.conv3 = nn.Conv2d(64, 64, 4)
        #self.conv4 = nn.Conv2d(64, 128, 4)
        #self.fc1 = nn.Linear(4608, 512)
        #self.fc2 = nn.Linear(512, 1)
        
        self.conv1 = nn.Conv2d(1, 16, 10) #input_channel, out_channel (n filters), kernel_size
        self.conv2 = nn.Conv2d(16, 32, 7)
        self.conv3 = nn.Conv2d(32, 32, 4)
        self.conv4 = nn.Conv2d(32, 64, 4)
        self.fc1 = nn.Linear(2304, 512)
        self.fc2 = nn.Linear(512, 1)

        # weight init
        #for m in self.modules():
        #    if isinstance(m, nn.Conv2d):
        #        nn.init.kaiming_normal(m.weight, mode='fan_in')
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal(m.weight, 0, 1e-2)
                nn.init.normal(m.bias, 0.5, 1e-2)
            elif isinstance(m, nn.Linear):
                nn.init.normal(m.weight, 0, 2e-1)
                nn.init.normal(m.bias, 0, 1e-2)

    def sub_forward(self, x):
        """
        Forward pass the input image through 1 subnetwork.
        Args
        ----
        - x: a Variable of size (B, C, H, W). Contains either the first or
          second image pair across the input batch.
        Returns
        -------
        - out: a Variable of size (B, 4096). The hidden vector representation
          of the input vector x.
        """
        out = F.relu(F.max_pool2d(self.conv1(x), 2))
        out = F.relu(F.max_pool2d(self.conv2(out), 2))
        out = F.relu(F.max_pool2d(self.conv3(out), 2))
        out = F.relu(self.conv4(out))
        
        #print(f"\n\nSHAPE DE SAIDA: {out.shape}\n\n")

        out = out.view(out.shape[0], -1)
        out = F.sigmoid(self.fc1(out))
        return out

    def forward(self, x1, x2):
        """
        Forward pass the input image pairs through both subtwins. An image
        pair is composed of a left tensor x1 and a right tensor x2.
        Concretely, we compute the component-wise L1 distance of the hidden
        representations generated by each subnetwork, and feed the difference
        to a final fc-layer followed by a sigmoid activation function to
        generate a similarity score in the range [0, 1] for both embeddings.
        Args
        ----
        - x1: a Variable of size (B, C, H, W). The left image pairs along the
          batch dimension.
        - x2: a Variable of size (B, C, H, W). The right image pairs along the
          batch dimension.
        Returns
        -------
        - probas: a Variable of size (B, 1). A probability scalar indicating
          whether the left and right input pairs, along the batch dimension,
          correspond to the same class. We expect the network to spit out
          values near 1 when they belong to the same class, and 0 otherwise.
        """
        # encode image pairs
        h1 = self.sub_forward(x1)
        h2 = self.sub_forward(x2)

        # compute l1 distance
        diff = torch.abs(h1 - h2)

        # score the similarity between the 2 encodings
        scores = self.fc2(diff)
        
        # return scores (without sigmoid) and use bce_with_logits
        # for increased numerical stability
        return scores